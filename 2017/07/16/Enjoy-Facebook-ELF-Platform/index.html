<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Enjoy Facebook ELF Platform | Gao Fangshu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Facebook 在7月6日开源了 ELF(Extensive, Lightweight and Flexible) 平台，ELF 对即时战略游戏模拟做了很多优化，其轻量级的 MiniRTS 模拟环境使开发增强学习算法变得更高效，更多介绍可以参考 GitHub 的 README (Documentation 还在每天更新，目前虽然内容不多，但可持续关注)，另外还有 Facebook 主页介绍 和">
<meta name="keywords" content="Python,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Enjoy Facebook ELF Platform">
<meta property="og:url" content="http://gaofangshu.com/blog/2017/07/16/Enjoy-Facebook-ELF-Platform/index.html">
<meta property="og:site_name" content="Gao Fangshu">
<meta property="og:description" content="Facebook 在7月6日开源了 ELF(Extensive, Lightweight and Flexible) 平台，ELF 对即时战略游戏模拟做了很多优化，其轻量级的 MiniRTS 模拟环境使开发增强学习算法变得更高效，更多介绍可以参考 GitHub 的 README (Documentation 还在每天更新，目前虽然内容不多，但可持续关注)，另外还有 Facebook 主页介绍 和">
<meta property="og:image" content="http://gaofangshu.com/blog/img/evaluate.png">
<meta property="og:image" content="http://gaofangshu.com/blog/img/valuation.gif">
<meta property="og:updated_time" content="2017-07-15T23:08:35.301Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enjoy Facebook ELF Platform">
<meta name="twitter:description" content="Facebook 在7月6日开源了 ELF(Extensive, Lightweight and Flexible) 平台，ELF 对即时战略游戏模拟做了很多优化，其轻量级的 MiniRTS 模拟环境使开发增强学习算法变得更高效，更多介绍可以参考 GitHub 的 README (Documentation 还在每天更新，目前虽然内容不多，但可持续关注)，另外还有 Facebook 主页介绍 和">
<meta name="twitter:image" content="http://gaofangshu.com/blog/img/evaluate.png">
  
  
    <link rel="icon" href="favicon.ico">
  
  <link href="//fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css">
  
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Gao Fangshu</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">It is our choices that show what we truly are.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-home-icon" class="nav-icon" href="/blog/"></a>
        
          <a id="nav-tags-icon" class="nav-icon" href="/blog/tags"></a>
        
          <a id="nav-archives-icon" class="nav-icon" href="/blog/archives"></a>
        
          <a id="nav-about-icon" class="nav-icon" href="/blog/about"></a>
        
        
      </nav>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Enjoy-Facebook-ELF-Platform" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Enjoy Facebook ELF Platform
    </h1>
  

      </header>
    
    <time class="article-date" datetime="2017-07-15T22:32:00.000Z" itemprop="datePublished">07-16-2017</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>Facebook 在7月6日开源了 ELF(Extensive, Lightweight and Flexible) 平台，ELF 对即时战略游戏模拟做了很多优化，其轻量级的 MiniRTS 模拟环境使开发增强学习算法变得更高效，更多介绍可以参考 GitHub 的 <a href="https://github.com/facebookresearch/ELF/blob/master/README.md" target="_blank" rel="external">README</a> (<a href="http://yuandong-tian.com/html_elf/#" target="_blank" rel="external">Documentation</a> 还在每天更新，目前虽然内容不多，但可持续关注)，另外还有 <a href="https://code.facebook.com/posts/132985767285406/introducing-elf-an-extensive-lightweight-and-flexible-platform-for-game-research/" target="_blank" rel="external">Facebook 主页介绍</a> 和 <a href="https://arxiv.org/abs/1707.01067" target="_blank" rel="external">arXiv paper</a> 可以阅读。<br><a id="more"></a></p>
<p>接下来将从配置环境、用 ELF 自带算法玩 MiniRTS 两个方面展开。</p>
<h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><p>由于 ELF 自带的增强学习代码基于只能在 Linux 和 macOS 运行的 <a href="http://pytorch.org/" target="_blank" rel="external">PyTorch</a>，所以本文采用 Ubuntu 16.04 LTS + Anaconda3 4.2.0 + PyTorch 的组合。</p>
<p>在配置 ELF 环境的过程中，还有许多细节 (坑) 需要注意：</p>
<ul>
<li><p>需要安装编译 C++ 的 Linux 库，比如 gcc(&gt;=4.9)，在 Linux 终端中运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install gcc</div></pre></td></tr></table></figure>
</li>
<li><p>需要安装 tbb 库，否则程序会报错，具体见 <a href="https://github.com/facebookresearch/ELF/issues/7" target="_blank" rel="external">Issue</a>，在 Linux 终端中运行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install libtbb-dev</div></pre></td></tr></table></figure>
</li>
<li><p>在可视化游戏界面的时候，需要用于前后端连接的库 zeromq 4.0.4 和 libczmq 3.0.2，因为没有比较细致的安装文档，摸索了比较久，有以下链接值得参考：</p>
<ul>
<li><a href="https://zeromq.github.io/zeromq4-1/" target="_blank" rel="external">Older 4.x ZeroMQ downloads</a></li>
<li><a href="https://tuananh.org/2015/06/16/how-to-install-zeromq-on-ubuntu/" target="_blank" rel="external">How to install ZeroMQ on Ubuntu</a></li>
<li><a href="https://stackoverflow.com/questions/41289619/how-to-install-zeromq-4-on-ubuntu-16-10-from-source" target="_blank" rel="external">How to install ZeroMQ 4 on Ubuntu 16.10 from source</a></li>
<li><a href="http://blog.csdn.net/qq_15437667/article/details/50752399" target="_blank" rel="external">Install ZeroMQ - CSDN</a></li>
<li><a href="http://zeromq.org/area:download" target="_blank" rel="external">libzmq download documentation</a></li>
<li><a href="http://czmq.zeromq.org/page:get-the-software" target="_blank" rel="external">CZMQ documentation</a></li>
<li><a href="http://blog.csdn.net/changqing5818/article/details/46916293" target="_blank" rel="external">Install CZMQ - CSDN</a></li>
</ul>
</li>
<li>安装 python-dev 库，对于Python 2 和 Python 3 分别在 Linux 终端输入：<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python-dev</div><div class="line">sudo apt-get install python3-dev</div></pre></td></tr></table></figure>
</li>
</ul>
<p>其他机器配置 ELF 也许会出现不一样的问题，首先看看 ELF 项目的 <a href="https://github.com/facebookresearch/ELF/issues" target="_blank" rel="external">Issues</a> 中有没有人报告相同问题，然后就是各种 Google。严格按照 ELF README.md 中的步骤和库的版本配置，可以降低报错的概率。</p>
<h2 id="用-ELF-玩-MiniRTS"><a href="#用-ELF-玩-MiniRTS" class="headerlink" title="用 ELF 玩 MiniRTS"></a>用 ELF 玩 MiniRTS</h2><p>要用 ELF 玩 MiniRTS，首先需要编译 MiniRTS，即在命令行输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd ./rts/game_MC</div><div class="line">make gen</div><div class="line">make</div></pre></td></tr></table></figure></p>
<p>生成 <code>minirts.so</code> 供其他 Python 程序调用。注意所有的编译都需要用相同版本的 Python，否则会报错。指定方法例如：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">PYTHON_CONFIG=/media/gaofangshu/Windows/Ubuntu/anaconda3/bin/python3.5-config make</div></pre></td></tr></table></figure></p>
<h3 id="如何开始训练"><a href="#如何开始训练" class="headerlink" title="如何开始训练"></a>如何开始训练</h3><p>训练模型的入口位于 <code>run.py</code>, ELF 平台默认的训练方式是基于命令行的，以本文环境为例，用 Actor-Critic 算法开始训练则在命令行输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">game=./rts/game_MC/game model=actor_critic model_file=./rts/game_MC/model python3.5 run.py --num_games 1024 --batchsize 128 --freq_update 50 --fs_opponent 20 --latest_start 500 --latest_start_decay 0.99 --opponent_type AI_SIMPLE --tqdm</div></pre></td></tr></table></figure></p>
<p>以上指令定义了游戏环境文件 <code>game=./rts/game_MC/game</code>，模型 <code>model=actor_critic</code> 以及模型所在文件 <code>model_file=./rts/game_MC/model</code>。之后含有 <code>--</code> 的指令则定义了各种参数，比如 <code>--num_games 1024</code> 定义并行运行的游戏数量为 1024；<code>--batchsize 128</code> 定义每批样本大小为 128；<code>--freq_update 50</code> 即训练 agent 的动作输入频率为 50；<code>--fs_opponent 20</code> 即对练的 AI 动作更新频率为 20；相比之下 AI 动作更新更快，这给了 AI 一定的反应优势；<code>--latest_start 500</code> 设定了 agent 开始训练的最晚帧数，随机延迟训练起始帧数是为了使 agent 学习到的开局更多样化，有利于探索各种策略；<code>--latest_start_decay 0.99</code> 则使每局的训练起始帧数随着学习进程逐步下降，原理类似于 ε-greedy 中随机探索概率的逐步下降，兼顾了探索与经验应用；<code>--opponent_type AI_SIMPLE</code> 选择了对手 AI 的类型；<code>--tqdm</code> 则是为了显示训练进度条。除此之外，还有其他训练选项可以定义，具体见对应的 <a href="https://github.com/facebookresearch/ELF/blob/a784175c6fa3ba587d2bb0d0b0311ce64f250e06/docs/source/utils.rst" target="_blank" rel="external">Utils Documentation</a>。</p>
<p>另外，也可以在各种 IDE 中通过运行 <code>run.py</code> 的代码来开始训练模型，这种方法优点在于便于 debug，但缺点是麻烦，需要在代码中修改各种参数的定义，不如命令行直接。</p>
<h3 id="如何继续训练"><a href="#如何继续训练" class="headerlink" title="如何继续训练"></a>如何继续训练</h3><p>在上面的训练过程进行时，ELF 会默认在每 5000 次迭代 (iteration) 后保存一次模型参数到 <code>.bin</code> 文件，如果要在之前的 <code>.bin</code> 文件基础上继续学习，则在训练的指令行后设定 <code>--load</code> 参数，比如，如果要基于 <code>save-62544.bin</code> 继续训练，则输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">game=./rts/game_MC/game model=actor_critic model_file=./rts/game_MC/model python3.5 run.py --num_games 1024 --batchsize 128 --freq_update 50 --fs_opponent 20 --latest_start 500 --latest_start_decay 0.99 --opponent_type AI_SIMPLE --tqdm --load save-62544.bin</div></pre></td></tr></table></figure></p>
<p>将会在终端中看到如下信息：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Namespace(T=6, actor_only=False, ai_type='AI_NN', batchsize=128, discount=0.99, entropy_ratio=0.01, epsilon=0.0, eval=False, freq_update=50, fs_ai=50, fs_opponent=20, game_multi=None, gpu=None, grad_clip_norm=None, greedy=False, handicap_level=0, latest_start=500, latest_start_decay=0.99, load='save-62544.bin', max_tick=30000, mcts_threads=64, min_prob=1e-06, num_episode=10000, num_games=1024, num_minibatch=5000, opponent_type='AI_SIMPLE', ratio_change=0, record_dir='./record', sample_node='pi', sample_policy='epsilon-greedy', save_dir=None, save_prefix='save', seed=0, simple_ratio=-1, tqdm=True, verbose_collector=False, verbose_comm=False, wait_per_group=False)</div><div class="line">Version:  f25f500a1422b657369d8d8b8c5725d5d74616d7_</div><div class="line">Num Actions:  9</div><div class="line">Num unittype:  6</div><div class="line">Load from save-62544.bin</div><div class="line"><span class="meta">79%</span>|██████████████████████████████▋        | 3941/5000 [04:16&lt;01:29, 11.82it/s]</div></pre></td></tr></table></figure></p>
<h3 id="如何评估模型"><a href="#如何评估模型" class="headerlink" title="如何评估模型"></a>如何评估模型</h3><p>评估模型即用训练好的模型与 AI 对抗，并计算打败 AI 的几率，这同样可以在终端中用命令行完成：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eval_only=1 game=./rts/game_MC/game model=actor_critic model_file=./rts/game_MC/model python3 run.py --batchsize 128 --fs_opponent 20 --latest_start 500 --latest_start_decay 0.99 --num_games 1024 --opponent_type AI_SIMPLE --stats winrate --num_eval 100 --tqdm --load save-62544.bin --eval_gpu 0</div></pre></td></tr></table></figure></p>
<p><code>eval_only=1</code> 即开启评估模式，<code>--num_eval 100</code> 定义了测试局数为 1000，其他参数与训练时基本一致。但要注意，对于只有 1 个 GPU 的设备，还需要设定评估时的 GPU 个数，即 <code>--eval_gpu 0</code>，因为 ELF 中默认评估模式是多 GPU 设备的。</p>
<p>评估结果显示如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Load from save-63362.bin</div><div class="line">Version:  f25f500a1422b657369d8d8b8c5725d5d74616d7_</div><div class="line">Num Actions:  9</div><div class="line">Num unittype:  6</div><div class="line">1003it [00:32, 32.09it/s]</div><div class="line">str_win_rate: [0] Win rate: 0.336 [341/673/1014], Best win rate: 0.336 [0]</div><div class="line">str_acc_win_rate: Accumulated win rate: 0.336 [341/673/1014]</div><div class="line">count: 0</div><div class="line">new_record: True</div><div class="line">best_win_rate: 0.33629191321495694</div><div class="line">Beginning stop all collectors ...</div><div class="line">Stop all game threads ...</div><div class="line">Beginning stop all collectors ...</div><div class="line">Stop all game threads ...</div></pre></td></tr></table></figure></p>
<p>即 <code>save-63362.bin</code> 模型对 AI 的胜率大致为 33.6%，如果手动对整个训练过程进行记录，可以得到如下胜率变化图：<br><img src="../../../../img/evaluate.png" width="80%" style="border-radius: 5px"><br>可以看出，随着游戏次数增加，胜率有所提高，但图中只训练了77轮 (每轮1024个游戏环境并行执行) ，还需要继续训练达到收敛。</p>
<h3 id="游戏可视化"><a href="#游戏可视化" class="headerlink" title="游戏可视化"></a>游戏可视化</h3><p>ELF 自带游戏可视化模块，以 MiniRTS 为例，基于之前生成的 <code>minirts.so</code>，进入 <code>backend</code> 文件夹：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd ./rts/backend</div><div class="line">make minirts GAME_DIR=../game_MC</div></pre></td></tr></table></figure></p>
<p>编译完成后生成 <code>minirts</code> 可执行文件，对它施加命令就可以开始可视化了：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd ./rts/backend</div><div class="line">./minirts selfplay --vis_after 0</div></pre></td></tr></table></figure></p>
<p>打开 <code>./rts/frontend/minirts.html</code> 就可以看到：<br><img src="../../../../img/valuation.gif" width="80%" style="border-radius: 5px"><br>这项功能仍有一些 bug，比如 Pause 键暂停后无法继续游戏等等。</p>
<h3 id="平台缺陷"><a href="#平台缺陷" class="headerlink" title="平台缺陷"></a>平台缺陷</h3><p>由于 ELF 开源距今只有 10 天，目前不可避免地有缺陷：</p>
<ul>
<li>源代码中仍然有 bug 需要自己调试或者提交 Issues 给开发者</li>
<li>对于自定义算法还没有接口，仍需要到 <code>./rlpytorch/rlmethod_common.py</code>, <code>./rts/game_MC/model.py</code> 等源文件中添加新的类修改代码，但 Facebook 团队回复 Issues 说之后会将模型调整为独立的包并完善接口</li>
<li>可视化界面目前只能用于人和自带 AI 的对战 (humanplay) 以及自带 AI 之间的对战 (selfplay)，目前没有对于训练模型的可视化，即无法基于 <code>.bin</code> 文件可视化游戏</li>
<li><a href="http://yuandong-tian.com/html_elf/#" target="_blank" rel="external">Documentation</a> 很不健全，还在每天上传完善</li>
</ul>
<p>总之，ELF 会是高效便捷的强化学习研究环境，但目前处于起步阶段，还需要对各个功能的更新持续关注 :D 。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://gaofangshu.com/blog/2017/07/16/Enjoy-Facebook-ELF-Platform/" data-id="cj629hlzb0006ncvg63gvg1g0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Python/">Python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2017/07/28/Play-Starcraft-with-Matrix/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Play Starcraft with Matrix
        
      </div>
    </a>
  
  
    <a href="/blog/2017/07/09/Linked-List-and-Chained-Assignment/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Linked List and Chained Assignment</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      Powered by <a href="http://hexo.io" target="_blank" title="Hexo">Hexo</a><br>
      ♡ <a href="http://xushuangblog.com/" target="_blank" title="Xu Shuang">Xu Shuang</a><br>
      &copy; 2016-2017 <a href="http://gaofangshu.com/blog/about/" target="_blank" title="Gao Fangshu">Gao Fangshu</a>
      <br>
    </div>
  </div>
</footer>
    </div>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/script.js"></script>

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  // ################### ADDED ON 2017.04.12 BY GAOFANGSHU ###################
  MathJax.Hub.Config({
    CommonHTML: {matchFontHeight: false},
    "HTML-CSS": {matchFontHeight: false},
    SVG: {matchFontHeight: false}
  // #########################################################################
  });
</script>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

  </div>
</body>
</html>