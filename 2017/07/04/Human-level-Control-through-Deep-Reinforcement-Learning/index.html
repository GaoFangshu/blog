<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Human-level Control through Deep Reinforcement Learning | Gao Fangshu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这篇来自 DeepMind 的文章将增强学习与神经网络结合，克服了之前增强学习“需要提炼关键信息或者提供明确的、有决定性的变量”这一缺陷，文中的模型可以利用神经网络直接从高维数据（游戏画面）中提炼关键信息，从而避免了 Q-learning 中需要学习记忆的情况太多以至于难以收敛的问题。">
<meta name="keywords" content="Machine Learning,Read Papers">
<meta property="og:type" content="article">
<meta property="og:title" content="Human-level Control through Deep Reinforcement Learning">
<meta property="og:url" content="http://gaofangshu.com/blog/2017/07/04/Human-level-Control-through-Deep-Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Gao Fangshu">
<meta property="og:description" content="这篇来自 DeepMind 的文章将增强学习与神经网络结合，克服了之前增强学习“需要提炼关键信息或者提供明确的、有决定性的变量”这一缺陷，文中的模型可以利用神经网络直接从高维数据（游戏画面）中提炼关键信息，从而避免了 Q-learning 中需要学习记忆的情况太多以至于难以收敛的问题。">
<meta property="og:updated_time" content="2017-07-09T06:10:35.573Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Human-level Control through Deep Reinforcement Learning">
<meta name="twitter:description" content="这篇来自 DeepMind 的文章将增强学习与神经网络结合，克服了之前增强学习“需要提炼关键信息或者提供明确的、有决定性的变量”这一缺陷，文中的模型可以利用神经网络直接从高维数据（游戏画面）中提炼关键信息，从而避免了 Q-learning 中需要学习记忆的情况太多以至于难以收敛的问题。">
  
  
    <link rel="icon" href="favicon.ico">
  
  <link href="//fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/blog/css/style.css">
  
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Gao Fangshu</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">It is our choices that show what we truly are.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="sub-nav">
        
          <a id="nav-home-icon" class="nav-icon" href="/blog/"></a>
        
          <a id="nav-tags-icon" class="nav-icon" href="/blog/tags"></a>
        
          <a id="nav-archives-icon" class="nav-icon" href="/blog/archives"></a>
        
          <a id="nav-about-icon" class="nav-icon" href="/blog/about"></a>
        
        
      </nav>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Human-level-Control-through-Deep-Reinforcement-Learning" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Human-level Control through Deep Reinforcement Learning
    </h1>
  

      </header>
    
    <time class="article-date" datetime="2017-07-04T13:33:51.000Z" itemprop="datePublished">07-04-2017</time>
    
  </div>
  <div class="article-inner">
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇来自 DeepMind 的文章将增强学习与神经网络结合，克服了之前增强学习“需要提炼关键信息或者提供明确的、有决定性的变量”这一缺陷，文中的模型可以利用神经网络直接从高维数据（游戏画面）中提炼关键信息，从而避免了 Q-learning 中需要学习记忆的情况太多以至于难以收敛的问题。<br><a id="more"></a></p>
<p>自然地，分析这篇文章的原理可以从两个角度出发，即增强学习（文中采用 Q-learning）和深度学习。</p>
<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>下面的公式是认识 Q-learning 的一个切入点：$$<br>\begin{align}<br>Q_{t+1}(s_{t},a_{t}) &amp;= Q_{t}(s_{t},a_{t})+\alpha\cdot(r_{t}+\gamma\cdot\max\limits_{a}Q(s_{t+1},a)-Q_{t}(s_{t},a_{t})) \\<br>&amp;= (1-\alpha)\cdot Q_{t}(s_{t},a_{t})+\alpha\cdot(r_{t}+\gamma\cdot\max\limits_{a}Q(s_{t+1},a))<br>\end{align}$$上式即表示了动作效用函数 $Q$ 的更新方式，由这期行动得到的未来最大回报基于当期回报 $r_{t}$ 和下一期情形下得到的回报 $\max\limits_{a}Q(s_{t+1},a)$。$Q_{t}(s_{t},a_{t})$ 是在情形 $s_{t}$ 下做出行动 $a_{t}$ 所获的预期回报，而 $r_{t}$ 是在情形 $s_{t}$ 下做出行动 $a_{t}$ 所获的实际回报，$\max\limits_{a}Q(s_{t+1},a)$ 是根据之前训练的记忆，在 $s_{t+1}$ 下做出选择所能获得最大回报。$\gamma$ 是未来回报的折现因子（discount factor）。$\alpha$ 是学习速率（learning rate），$\alpha$ 越大，保留之前训练的效果就越少。$r_{t}+\gamma\cdot\max\limits_{a}Q(s_{t+1},a)$ 则是每次被学习到的部分。</p>
<p>在学习开始前，$Q$ 是一个人为设定的值，每次 agent 选择一个行动并观察到回报，新的情形也就来自于上一个情形以及当下的行动，从而将 $Q$ 更新。<br>用伪代码表示即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">initialize Q    // 随机初始化Q</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> M:    // 每一局游戏为一个episode</div><div class="line">    initialize S    // S为游戏开始的初始情形</div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> T:    // 直到游戏结束</div><div class="line">        choose action a_t    // 选择一个行动，比如以文中的 ε-greedy 策略，以概率 ε 选择随机行动，概率 <span class="number">1</span>-ε 选择当前状态下效用值最大的行动</div><div class="line">        get r_t, x_t+<span class="number">1</span>, s_t+<span class="number">1</span>    // 执行 a_t 后，得到对应回报 r_t 和新的画面 x_t+<span class="number">1</span>，情形更新为 s_t+<span class="number">1</span></div><div class="line">        Q(s_t,a_t) = Q(s_t,a_t) + α*[r_t + γ*maxQ(S,a) - Q(s_t,a_t)]    // 在maxQ(S,a)中更新S=s_t+<span class="number">1</span>，得到新的Q</div><div class="line">        S = s_t+<span class="number">1</span>    // 情形更新</div></pre></td></tr></table></figure>
<h2 id="神经网络的引入"><a href="#神经网络的引入" class="headerlink" title="神经网络的引入"></a>神经网络的引入</h2><p>对于本文的任务，单纯使用 Q-learning 增强学习有两个问题：一个是 Q-learning 需要针对不同的游戏各自设定能刻画情形 $s$ 的指标，即无法用同一个 Q-learning 学习不同的游戏；另一个是如果将 84×84×4 的游戏画面直接作为输入，那么 Q-table 中就有太多太多的情形（对于 256 色的屏幕，则有 $256^{84\times84\times4}$ 种）需要列举学习，这在运算上根本不现实。因此，DeepMind 将深度学习引入原来的增强学习模型，来替代“屏幕输入 - $Q$ 输出”这一过程，神经网络架构如下：</p>
<div align="center"><table width="80%"><tr><th style="text-align:center">Layer</th><th style="text-align:center">Input</th><th style="text-align:center">Filtersize</th><th style="text-align:center">Stride</th><th style="text-align:center">Numfilters</th><th style="text-align:center">Activation</th><th style="text-align:center">Output</th></tr><tr><td style="text-align:center">conv1</td><td style="text-align:center">84×84×4</td><td style="text-align:center">8×8</td><td style="text-align:center">4</td><td style="text-align:center">32</td><td style="text-align:center">ReLU</td><td style="text-align:center">20×20×32</td></tr><tr><td style="text-align:center">conv2</td><td style="text-align:center">20×20×32</td><td style="text-align:center">4×4</td><td style="text-align:center">2</td><td style="text-align:center">64</td><td style="text-align:center">ReLU</td><td style="text-align:center">9×9×64</td></tr><tr><td style="text-align:center">conv3</td><td style="text-align:center">9×9×64</td><td style="text-align:center">3×3</td><td style="text-align:center">1</td><td style="text-align:center">64</td><td style="text-align:center">ReLU</td><td style="text-align:center">7×7×64</td></tr><tr><td style="text-align:center">fc4</td><td style="text-align:center">7×7×64</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">512</td><td style="text-align:center">ReLU</td><td style="text-align:center">512</td></tr><tr><td style="text-align:center">fc5</td><td style="text-align:center">512</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">18</td><td style="text-align:center">Linear</td><td style="text-align:center">18</td></tr></table></div>

<p>网络的输入是 4 幅 84×84 的画面，输出是可能的 18 个行动对应的 $Q$（选择其中最大值对应的行动），其中损失函数为：$$L=[r_{t}+\gamma\cdot\max\limits_{a}Q(s_{t+1},a)-Q_{t}(s_{t},a_{t})]^2$$由于 $Q$ 有可能是任何大小的值，因此上面的损失函数看重的是行动得到的 $r_{t}+\gamma\cdot\max\limits_{a}Q(s_{t+1},a)$ 对原来预测 $Q_{t}(s_{t},a_{t})$ 的偏离情况。</p>
<p>综上，本文的 deep Q-learning 用伪代码表示即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">initialize D    // 初始化记忆空间</div><div class="line">initialize Q    // 随机初始化Q</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> M:    // 每一局游戏为一个episode</div><div class="line">    initialize S    // S为游戏开始的初始情形</div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> T:    // 直到游戏结束</div><div class="line">	    choose action a_t    // 以 ε-greedy 策略选择一个行动</div><div class="line">            get r_t, x_t+<span class="number">1</span>, s_t+<span class="number">1</span>    // 执行 a_t 后，得到对应回报 r_t 和新的画面 x_t+<span class="number">1</span>，情形更新为 s_t+<span class="number">1</span></div><div class="line">	    update D    // 更新记忆空间D，添加经验(s_t, a_t, r_t, s_t+<span class="number">1</span>)</div><div class="line">	    sample random (s0, a0, r0, s0<span class="string">') from D   // 从D中随机抽取经验(s0, a0, r0, s0'</span>)</div><div class="line">	    <span class="keyword">if</span> game end at s<span class="string">':</span></div><div class="line"><span class="string">	        y = r0    // 如果s0'</span>是游戏终止</div><div class="line">	    <span class="keyword">else</span>:</div><div class="line">                y = r0 + γmaxQ(s0<span class="string">',a)    // 如果s0'</span>不是游戏终止,找到使Q最大的行动a</div><div class="line">	    train the network using L = (y - Q(s0,a0))^<span class="number">2</span>    // 按照损失函数L进行梯度下降</div><div class="line">            S = s_t+<span class="number">1</span>    // 情形更新，论文中为每C步更新一次</div></pre></td></tr></table></figure>
<h2 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h2><h3 id="Experience-replay"><a href="#Experience-replay" class="headerlink" title="Experience replay"></a>Experience replay</h3><p>Experience replay 即从过往保存的记忆数据中随机抽取子样本用于训练，相比直接将上一次行动的经验用于训练，这种方法减少了训练数据的序列相关性，可以避免训练陷入局部最优。</p>
<h3 id="ε-greedy"><a href="#ε-greedy" class="headerlink" title="ε-greedy"></a>ε-greedy</h3><p>为了解决“探索更优行动”和“直接采用现有经验”之间的矛盾（exploration-exploitation dilemma），本文采用了 ε-greedy 策略，以概率 $\varepsilon$ 选择随机行动（探索），概率 $1-\varepsilon$ 选择当前状态下效用值最大的行动（直接采用经验，即用 $\arg\max\limits_{a}Q(s_{t},a)$）。并且在训练开始时 $\varepsilon=1$，并逐渐随着训练降低至 $\varepsilon=0.1$，在这种策略下，训练开始时可以探索到尽可能多的情形，在后期侧重经验又可以保证游戏表现。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://gaofangshu.com/blog/2017/07/04/Human-level-Control-through-Deep-Reinforcement-Learning/" data-id="cj6294iyl000864vgd8w7gk4d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Read-Papers/">Read Papers</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2017/07/09/Linked-List-and-Chained-Assignment/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Linked List and Chained Assignment
        
      </div>
    </a>
  
  
    <a href="/blog/2017/05/04/Number-Classification-with-MNIST-Data/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Number Classification with MNIST Data</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      Powered by <a href="http://hexo.io" target="_blank" title="Hexo">Hexo</a><br>
      ♡ <a href="http://xushuangblog.com/" target="_blank" title="Xu Shuang">Xu Shuang</a><br>
      &copy; 2016-2017 <a href="http://gaofangshu.com/blog/about/" target="_blank" title="Gao Fangshu">Gao Fangshu</a>
      <br>
    </div>
  </div>
</footer>
    </div>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">
  <script src="/blog/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog/js/script.js"></script>

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  // ################### ADDED ON 2017.04.12 BY GAOFANGSHU ###################
  MathJax.Hub.Config({
    CommonHTML: {matchFontHeight: false},
    "HTML-CSS": {matchFontHeight: false},
    SVG: {matchFontHeight: false}
  // #########################################################################
  });
</script>

<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

  </div>
</body>
</html>